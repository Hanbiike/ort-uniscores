import os
import glob
import asyncio
import aiohttp
from bs4 import BeautifulSoup
from urllib.parse import urlparse, unquote

OUTPUT_DIR = "downloaded"
os.makedirs(OUTPUT_DIR, exist_ok=True)


def make_filename(url: str) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ–º .html"""
    parsed = urlparse(url)

    # –ë–µ—Ä—ë–º –ø—É—Ç—å (–±–µ–∑ –≤–µ–¥—É—â–µ–≥–æ /)
    path = parsed.path.strip("/")
    if not path:
        path = "index"

    # –î–æ–±–∞–≤–ª—è–µ–º query (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if parsed.query:
        path += "_" + parsed.query.replace("=", "-").replace("&", "_")

    # –î–µ–ª–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è
    safe_name = unquote(path).replace("/", "_").replace("?", "_")

    # –ï—Å–ª–∏ –∏–º—è –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º .html
    if not safe_name.lower().endswith(".html"):
        safe_name += ".html"

    return safe_name


async def fetch(session, url):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ"""
    filename = make_filename(url)
    local_path = os.path.join(OUTPUT_DIR, filename)

    try:
        async with session.get(url, timeout=20) as resp:
            if resp.status == 200:
                text = await resp.text()
                with open(local_path, "w", encoding="utf-8") as f:
                    f.write(text)
                print(f"‚¨áÔ∏è {url} ‚Üí {local_path}")
                return url, local_path
            else:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ {resp.status} –¥–ª—è {url}")
                return url, None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")
        return url, None


async def download_all(urls):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏"""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return {url: path for url, path in results if path}


def extract_links(files):
    """–î–æ—Å—Ç–∞—ë—Ç –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∏–∑ report*.html"""
    links = set()
    for filepath in files:
        with open(filepath, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
            for a in soup.find_all("a", href=True):
                links.add(a["href"])
    return links


def rewrite_reports(files, link_map):
    """–ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã"""
    for filepath in files:
        with open(filepath, "r", encoding="utf-8") as f:
            soup = BeautifulSoup(f, "html.parser")
            for a in soup.find_all("a", href=True):
                if a["href"] in link_map:
                    a["href"] = link_map[a["href"]]
        new_name = filepath.replace(".html", "_local.html")
        with open(new_name, "w", encoding="utf-8") as f:
            f.write(str(soup))
        print(f"üîó –ü–µ—Ä–µ–ø–∏—Å–∞–Ω {filepath} ‚Üí {new_name}")


async def main():
    report_files = glob.glob("report*.html")
    links = extract_links(report_files)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫")

    # –°–∫–∞—á–∏–≤–∞–Ω–∏–µ
    link_map = await download_all(links)
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ {len(link_map)} —Ñ–∞–π–ª–æ–≤")

    # –ü–µ—Ä–µ–∑–∞–ø–∏—Å—å —Å—Å—ã–ª–æ–∫
    rewrite_reports(report_files, link_map)


if __name__ == "__main__":
    asyncio.run(main())